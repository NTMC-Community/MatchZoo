"""An implementation of MvLstm Model."""
from matchzoo import engine
import typing
import numpy as np
from keras.models import Model
import keras.backend as K
from keras.layers.wrappers import Bidirectional
from keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Dropout, \
    Concatenate, Flatten, LSTM, Dot, Reshape, Lambda, Flatten, Dense


class MvLstmModel(engine.BaseModel):
    """
    MvLstm Model.

    Examples:
        >>> model = MvLstmModel()
        >>> model.guess_and_fill_missing_params()
        >>> model.build()

    """

    @classmethod
    def get_default_params(cls) -> engine.ParamTable:
        """:return: model default parameters."""
        params = super().get_default_params()
        params['optimizer'] = 'adam'
        params['input_shapes'] = [(32,), (32,)]
        params.add(engine.Param('trainable_embedding', False))
        params.add(engine.Param('embedding_dim', 300))
        params.add(engine.Param('vocab_size', 100))
        params.add(engine.Param('hidden_size', 32))
        params.add(engine.Param('topk', 10))
        params.add(engine.Param('padding', 'same'))
        params.add(engine.Param('dropout_rate', 0.0))
        params.add(engine.Param('embedding_mat', None))
        params.add(engine.Param('embedding_random_scale', 0.2))
        return params

    def _conv_pool_block(self, input: typing.Any, kernel_count: int,
                         kernel_size: int, padding: str, activation: str,
                         pool_size: int) -> typing.Any:
        output = Conv1D(kernel_count,
                        kernel_size,
                        padding=padding,
                        activation=activation)(input)
        output = MaxPooling1D(pool_size=pool_size)(output)
        return output

    @property
    def embedding_mat(self) -> np.ndarray:
        """Get pretrained embedding for MvLstm model."""
        # Check if provided embedding matrix
        if self._params['embedding_mat'] is None:
            s = self._params['embedding_random_scale']
            self._params['embedding_mat'] = \
                np.random.uniform(-s, s, (self._params['vocab_size'],
                                          self._params['embedding_dim']))
        return self._params['embedding_mat']

    @embedding_mat.setter
    def embedding_mat(self, embedding_mat: np.ndarray):
        """
        Set pretrained embedding for MvLstm model.

        :param embedding_mat: pretrained embedding in numpy format.
        """
        self._params['embedding_mat'] = embedding_mat
        self._params['vocab_size'], self._params['embedding_dim'] = \
            embedding_mat.shape

    def build(self):
        """
        Build model structure.

        MvLstm use Siamese arthitecture.
        """
        print('------------------input')
        print(self._params['input_shapes'])
        # Left input and right input.
        input_left = Input(name='text_left',
                           shape=self._params['input_shapes'][0])
        input_right = Input(name='text_right',
                            shape=self._params['input_shapes'][1])
        # Process left & right input.
        embedding = Embedding(self._params['vocab_size'],
                              self._params['embedding_dim'],
                              weights=[self.embedding_mat],
                              trainable=self._params['trainable_embedding'])

        input_left = embedding(input_left)
        input_right = embedding(input_right)

#        bi_lstm = Bidirectional(LSTM(self._params['hidden_size'],
#                              return_sequences=True,
#                              dropout=self._params['dropout_rate']))
        rep_left = Bidirectional(LSTM(self._params['hidden_size'],
                              return_sequences=True,
                              dropout=self._params['dropout_rate']))(input_left)
        rep_right = Bidirectional(LSTM(self._params['hidden_size'],
                              return_sequences=True,
                              dropout=self._params['dropout_rate']))(input_right)
        #rep_left = bi_lstm(input_left)
        #rep_right = bi_lstm(input_right)
        print('----------------re_right:')
        #print(K.tf.shape(rep_right))
        print(rep_right.shape)
        x = Dot(axes=[1, 1], normalize=False)([rep_left, rep_right])
        print('----------------dot:')
        #print(K.tf.shape(x))
        print(x.shape)
        x = Reshape((-1, ))(x)
        #mm_k = x
        print('----------------reshape:')
        print(x.shape)


        mm_k = Lambda(lambda x: K.tf.nn.top_k(x,
                             k=self._params['topk'],
                             sorted=True)[0])(x)
        print('----------------topk:')
        print(mm_k.shape)
        mm_k = Dropout(rate=self._params['dropout_rate'])(mm_k)

        #embed_flat = Flatten()(Concatenate(axis=1)([embed_left, embed_right]))
        #mm_k = Flatten()(mm_k)

        #mm_k = Flatten()(mm_k)
        x_out = self._make_output_layer()(mm_k)
        #x_out = Dense(1)(x_out)
        #mm_k = Flatten()(input_right)
        #mm_k = input_right
        #x_out = Dense(1)(mm_k)
        self._backend = Model(
            inputs=[input_left, input_right],
            outputs=x_out)
