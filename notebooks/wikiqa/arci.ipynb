{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Arc-I Model\n",
    "\n",
    "<img src=\"https://github.com/faneshion/MatchZoo/blob/master/docs/_static/images/matchzoo-logo.png?raw=true\" alt=\"logo\" style=\"width:600px;float: center\"/>\n",
    "\n",
    "This is a tutorial on training *Arc-I Model* [Hu et al. 2014](http://papers.nips.cc/paper/5550-convolutional-neural-network-architectures-for-matching-natural-language-sentences.pdf) model with [MatchZoo](https://github.com/faneshion/MatchZoo). We use [WikiQA](https://aclweb.org/anthology/D15-1237) as the example benchmark data set to show the usage.\n",
    "\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR**\n",
    "\n",
    "The following code block illustrates the main workflow of how to train a Arc-I model. \n",
    "\n",
    "```python\n",
    "from matchzoo import preprocessor\n",
    "from matchzoo import generators\n",
    "from matchzoo import models\n",
    "\n",
    "train, test = ... # prepare your training data and test data.\n",
    "\n",
    "arci_preprocessor = preprocessor.ArcIPreprocessor()\n",
    "processed_tr = arci_preprocessor.fit_transform(train, stage='train')\n",
    "processed_te = arci_preprocessor.fit_transform(test, stage='test')\n",
    "\n",
    "input_shapes = processed_tr.context['input_shapes']\n",
    "\n",
    "generator_tr = generators.PointGenerator(processed_tr)\n",
    "generator_te = generators.PointGenerator(processed_te)\n",
    "# Example, train with generator, test with the first batch.\n",
    "X_te, y_te = generator_te[0]\n",
    "\n",
    "arci_model = models.ArcIModel()\n",
    "arci_model.params['input_shapes'] = input_shapes\n",
    "arci_model.guess_and_fill_missing_params()\n",
    "arci_model.build()\n",
    "arci_model.compile()\n",
    "arci_model.fit_generator(generator_tr)\n",
    "# Make predictions\n",
    "predictions = arci_model.predict([X_te.text_left, X_te.text_right])\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MatchZoo expect a list of *Quintuple* as training data:\n",
    "\n",
    "```python\n",
    "train = [('qid0', 'did0', 'query 0', 'document 0', 'label 0'),\n",
    "         ('qid0', 'did1', 'query 0', 'document 1', 'label 1'),\n",
    "          ...,\n",
    "         ('qid1', 'did2', 'query 1', 'document 2', 'label 3')]\n",
    "```\n",
    "\n",
    "The corresponded columns are `(text_left_id, text_right_id, text_left, text_right, label)`. For Information Retrieval task, *text_left* is referred as *query*, and *text_right* is document.\n",
    "\n",
    "For the test case, MatchZoo expect a list of *Quadruple* (we do not need labels) as input:\n",
    "\n",
    "```python\n",
    "test = [('qid9', 'did5', 'query 9', 'document 5'),\n",
    "         ...,\n",
    "        ('qid2', 'did7', 'query 2', 'document 7')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "\n",
    "+ Prepare WikiQA dataset\n",
    "    - Download\n",
    "    - Load\n",
    "    - Adjustment\n",
    "+ Preprocessing\n",
    "+ Data Generator\n",
    "+ Model Training\n",
    "    - Initialize\n",
    "    - Hyper-Parameters\n",
    "    - Make Prediction\n",
    "    - Model Persistence\n",
    "- Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare WikiQA dataset\n",
    "\n",
    "#### Download\n",
    "\n",
    "We take WikiQA as the example benchmark dataset to show the usage of MatchZoo. Firstly you need to downlowd the data and uncompress the data, we provided the following script to help you download the dataset into `MatchZoo/data/WikiQA` folder, you can change the directory in the following script.\n",
    "\n",
    "If you already have WikiQA dataset downloaded on your machine, skip the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download WikiQA data...  rm -rf ../../data/WikiQA\n",
      "mkdir -p ../../data/WikiQA/\n",
      "cd ../../data/WikiQA/\n",
      "wget https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
      "unzip WikiQACorpus.zip\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cmd = 'rm -rf ../../data/WikiQA\\n' \\\n",
    "      +'mkdir -p ../../data/WikiQA/\\n' \\\n",
    "      +'cd ../../data/WikiQA/\\n' \\\n",
    "      +'wget https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\\n' \\\n",
    "      +'unzip WikiQACorpus.zip\\n'\n",
    "print ('download WikiQA data... ', cmd)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Adjustment\n",
    "\n",
    "The *train/dev/test* files of WikiQA are *WikiQA-train.tsv*, *WikiQA-dev.tsv*, *WikiQA-test.tsv* under the uncompressed folder WikiQACorpus. The data format of WikiQA is as follows:\n",
    "\n",
    "`QuestionID\\tQuestion\\tDocumentID\\tDocumentTitle\\tSentenceID\\tSentence\\tLabel`\n",
    "\n",
    "We can convert this format to the expected input format of MatchZoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = '../../data/WikiQA/WikiQACorpus/'\n",
    "\n",
    "def read_data(input, stage):\n",
    "    output_list = []\n",
    "    index = 0\n",
    "    with open(input) as fin:\n",
    "        for l in fin:\n",
    "            tok = l.split('\\t')\n",
    "            if index != 0:\n",
    "                if stage == 'test':\n",
    "                    output_list.append((tok[0], tok[4], tok[1], tok[5])) # qid, did, q, d, label\n",
    "                else:\n",
    "                    output_list.append((tok[0], tok[4], tok[1], tok[5], tok[6])) # qid, did, q, d \n",
    "            index += 1\n",
    "    return output_list\n",
    "\n",
    "train = read_data(data_folder + 'WikiQA-train.tsv', stage='train')\n",
    "dev   = read_data(data_folder + 'WikiQA-dev.tsv', stage='dev')\n",
    "test  = read_data(data_folder + 'WikiQA-test.tsv', stage='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "You can pre-process your DSSM input in three lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start building vocabulary & fitting parameters.\n",
      "2118it [00:00, 2496.62it/s]\n",
      "18841it [00:09, 1888.30it/s]\n",
      "Start processing input data for train stage.\n",
      "2118it [00:00, 2446.24it/s]\n",
      "18841it [00:10, 1758.83it/s]\n",
      "Start processing input data for test stage.\n",
      "633it [00:00, 2737.32it/s]\n",
      "5961it [00:03, 1913.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dssm preprocessor.\n",
    "from matchzoo import preprocessor\n",
    "arci_preprocessor = preprocessor.ArcIPreprocessor()\n",
    "processed_tr = arci_preprocessor.fit_transform(train, stage='train')\n",
    "processed_te = arci_preprocessor.fit_transform(test, stage='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `processed_tr`?**\n",
    "\n",
    "`processed_tr` is a **MatchZoo DataPack** data structure (see `matchzoo/datapack.py`). It contains \n",
    "1. A *2-columns* `pandas DataFrame` to host all the pre-processed records including index and processed text.\n",
    "2. A `mapping` variable (python `dict`) to store the relationship between id pairs.\n",
    "2. a `context` property (dictionary) consists of all the parameters fitted during pre-processing. \n",
    "\n",
    "The `fit_transform` method is a linear combination of two methods:\n",
    "\n",
    "1. Fit parameters using the `fit` function, this only happens when `stage='train'`.\n",
    "2. Transform data into expected format.\n",
    "\n",
    "So the previous preprocessing code can also be written as:\n",
    "\n",
    "```python\n",
    "processed_te = dssm_preprocessor.transform(test, stage='test')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described, the fitted parameters were stored in `context` property, to access the context, just call:\n",
    "\n",
    "```python\n",
    "print(processed_tr.context)\n",
    "```\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['term_index', 'input_shapes'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tr.context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  29924\n"
     ]
    }
   ],
   "source": [
    "print('vocab size: ', len(processed_tr.context['term_index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What has been stored in the `context?`** \n",
    "\n",
    "We stored `term_index` and `input_shapes` in the context property. \n",
    "\n",
    "\n",
    "**What is `arci_preprocessor` actually doing?**\n",
    "\n",
    "The `arci_preprocessor` is calling a sequence of `process_units`. Each `process_unit` is designed to perform one atom operation on input data. For instance, in `arci_preprocessor`, we called:\n",
    "\n",
    "\n",
    "1. TokenizeUnit: Perform tokenization on raw input data.\n",
    "2. LowercaseUnit: Transform all tokens into lower case.\n",
    "3. PuncRemovalUnit: Remove all the punctuations.\n",
    "4. StopRemovalUnit: Remove all the stopwords.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### Data Generation\n",
    "\n",
    "For memory efficiency, we expect you to use **generator** to generate batches of data on the fly. For example, we can create a **PointGenerator** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matchzoo import generators\n",
    "from matchzoo import tasks\n",
    "generator_tr = generators.PointGenerator(inputs=processed_tr, task=tasks.Ranking(), batch_size=64, stage='train')\n",
    "generator_te = generators.PointGenerator(inputs=processed_te, task=tasks.Ranking(), batch_size=64, stage='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first batch of trainig data, just call `X_train, y_train = generator[0]`.\n",
    "\n",
    "**What is PointGenerator?**\n",
    "**PointGenerator** is this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the problem can be approximated by a regression/Classification problem — given a single query-document pair, predict its score.\n",
    "\n",
    "A number of existing supervised machine learning tasks fall into this line. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair.\n",
    "\n",
    "**What is PairGenerator?**\n",
    "In this case, the problem is approximated by a classification problem — learning a binary classifier that can tell which document is better in a given pair of documents.\n",
    "\n",
    "In MatchZoo, **PairGenerator** generate one positive & `num_neg` negative examples per pair. As an example, to train a DSSM model (for document ranking), we use `num_neg=4`. \n",
    "\n",
    "**What is ListGenerator?**\n",
    "This generator try to directly optimize the value of evaluation measures, averaged over all queries in the training data. \n",
    "\n",
    "Chosse the appropriate generator based on your `task`.\n",
    "\n",
    "----\n",
    "\n",
    "### Train Your Arc-I Model\n",
    "\n",
    "To train a Arc-I model, we need to create an instance of ArcIModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matchzoo import models\n",
    "arci_model = models.ArcIModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The fitted parameters is stored in the `context` property of pre-processor instance during the training stage.\n",
    "from matchzoo import losses\n",
    "from matchzoo import tasks\n",
    "input_shapes = processed_tr.context['input_shapes']\n",
    "arci_model.params['task'] = tasks.Ranking()\n",
    "arci_model.params['vocab_size'] = len(processed_tr.context['term_index']) + 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arci parameters:  name                          ArcIModel\n",
      "model_class                   <class 'matchzoo.models.arci_model.ArcIModel'>\n",
      "input_shapes                  [(32,), (32,)]\n",
      "task                          <matchzoo.tasks.ranking.Ranking object at 0x11121f278>\n",
      "metrics                       ['mae']\n",
      "loss                          mse\n",
      "optimizer                     adam\n",
      "trainable_embedding           False\n",
      "embedding_dim                 300\n",
      "vocab_size                    29925\n",
      "num_blocks                    1\n",
      "left_kernel_count             [32]\n",
      "left_kernel_size              [3]\n",
      "right_kernel_count            [32]\n",
      "right_kernel_size             [3]\n",
      "activation                    relu\n",
      "left_pool_size                [2]\n",
      "right_pool_size               [2]\n",
      "padding                       same\n",
      "dropout_rate                  0.0\n",
      "embedding_mat                 None\n",
      "embedding_random_scale        0.2\n"
     ]
    }
   ],
   "source": [
    "arci_model.guess_and_fill_missing_params()\n",
    "print('arci parameters: ', arci_model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "\n",
    "To train the model after all the parameters were settled, call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.0537 - mean_absolute_error: 0.1181\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.0445 - mean_absolute_error: 0.1002\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.0413 - mean_absolute_error: 0.1008\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.0336 - mean_absolute_error: 0.0980\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.0347 - mean_absolute_error: 0.1033\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.0278 - mean_absolute_error: 0.0985\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 7s 34ms/step - loss: 0.0254 - mean_absolute_error: 0.0965\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 8s 39ms/step - loss: 0.0248 - mean_absolute_error: 0.0984: 0s - loss: 0.0251 - mean_absolute_error: 0.0\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.0181 - mean_absolute_error: 0.0853\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 8s 38ms/step - loss: 0.0190 - mean_absolute_error: 0.0872\n"
     ]
    }
   ],
   "source": [
    "arci_model.build()\n",
    "arci_model.compile()\n",
    "# Fit the arci model on generator.\n",
    "arci_model.fit_generator(generator_tr, steps_per_epoch=200, epochs=10)\n",
    "# Make predictions on the first batch of test data\n",
    "X_te, y_te = generator_te[0]\n",
    "predictions = arci_model.predict([X_te.text_left, X_te.text_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q933/D901-9 is predicted as [-0.03582207]\n",
      "Q2304/D359-0 is predicted as [-0.09047449]\n",
      "Q2100/D1980-3 is predicted as [0.14439546]\n",
      "Q2382/D230-2 is predicted as [0.10213326]\n",
      "Q1485/D1411-5 is predicted as [0.27780926]\n",
      "Q2061/D1946-2 is predicted as [-0.11321834]\n",
      "Q340/D339-7 is predicted as [-0.01285515]\n",
      "Q33/D33-7 is predicted as [0.16359125]\n",
      "Q1957/D1847-8 is predicted as [0.09576143]\n",
      "Q1389/D1326-14 is predicted as [0.00998938]\n",
      "Q1446/D1376-8 is predicted as [0.291522]\n",
      "Q1899/D1795-26 is predicted as [0.08655713]\n",
      "Q285/D284-4 is predicted as [0.28293288]\n",
      "Q1983/D1870-4 is predicted as [-0.03440011]\n",
      "Q2286/D2151-0 is predicted as [-0.03345742]\n",
      "Q269/D268-3 is predicted as [0.15398271]\n",
      "Q20/D20-3 is predicted as [0.00160372]\n",
      "Q2801/D2602-0 is predicted as [0.25547576]\n",
      "Q2148/D2025-2 is predicted as [-0.12991714]\n",
      "Q995/D960-0 is predicted as [-0.17178614]\n",
      "Q337/D336-4 is predicted as [0.0780839]\n",
      "Q1216/D1165-0 is predicted as [0.03036081]\n",
      "Q1516/D557-12 is predicted as [0.09236333]\n",
      "Q506/D499-1 is predicted as [0.20876557]\n",
      "Q2753/D2563-1 is predicted as [-0.19274063]\n",
      "Q1758/D1665-9 is predicted as [0.22111784]\n",
      "Q1892/D1789-4 is predicted as [0.05191045]\n",
      "Q2198/D2071-1 is predicted as [-0.0168246]\n",
      "Q1564/D1482-3 is predicted as [-0.07629316]\n",
      "Q2100/D1980-5 is predicted as [-0.12602851]\n",
      "Q1175/D1128-3 is predicted as [-0.09493066]\n",
      "Q312/D311-5 is predicted as [-0.01964978]\n",
      "Q222/D222-1 is predicted as [-0.0695096]\n",
      "Q2741/D1331-19 is predicted as [0.07208928]\n",
      "Q1012/D976-14 is predicted as [0.10993412]\n",
      "Q2574/D2406-1 is predicted as [-0.01645707]\n",
      "Q952/D920-1 is predicted as [-0.03341964]\n",
      "Q1655/D1572-6 is predicted as [0.20040634]\n",
      "Q1564/D1482-6 is predicted as [0.10203353]\n",
      "Q1089/D1048-4 is predicted as [0.00925992]\n",
      "Q2080/D1963-9 is predicted as [0.04371078]\n",
      "Q2842/D2639-8 is predicted as [-0.01936167]\n",
      "Q1559/D1477-12 is predicted as [-0.14361121]\n",
      "Q735/D715-0 is predicted as [-0.20077989]\n",
      "Q243/D243-8 is predicted as [0.02297359]\n",
      "Q309/D308-13 is predicted as [0.17690383]\n",
      "Q2201/D2074-14 is predicted as [0.04456634]\n",
      "Q1154/D1110-15 is predicted as [0.10488535]\n",
      "Q1093/D1052-8 is predicted as [0.16786848]\n",
      "Q910/D879-2 is predicted as [-0.1039422]\n",
      "Q1749/D1656-4 is predicted as [-0.01644617]\n",
      "Q182/D182-7 is predicted as [0.01348259]\n",
      "Q102/D102-2 is predicted as [0.07800619]\n",
      "Q394/D392-0 is predicted as [0.37530392]\n",
      "Q455/D448-8 is predicted as [-0.03011321]\n",
      "Q384/D382-5 is predicted as [-0.08528405]\n",
      "Q1206/D1155-8 is predicted as [-0.10549048]\n",
      "Q1027/D990-10 is predicted as [0.12655011]\n",
      "Q2766/D1764-18 is predicted as [0.00718137]\n",
      "Q1675/D1591-7 is predicted as [-0.05318696]\n",
      "Q2980/D2753-19 is predicted as [0.10052191]\n",
      "Q1312/D1256-3 is predicted as [0.11833443]\n",
      "Q2803/D2604-3 is predicted as [-0.00573153]\n",
      "Q1077/D1038-7 is predicted as [0.00624939]\n"
     ]
    }
   ],
   "source": [
    "for id_left, id_right, pred in zip(X_te.id_left, X_te.id_right, predictions):\n",
    "    print(\"{}/{} is predicted as {}\".format(id_left, id_right, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Persistence\n",
    "\n",
    "You can persist your trained model using `model.save()` and `load_model` function:\n",
    "\n",
    "```python\n",
    "from matchzoo import engine\n",
    "# Save the model to dir.\n",
    "arci_model.save('/your-model-saved-path')\n",
    "# And load the model from dir.\n",
    "engine.load_model('/your-model-saved-path')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[Hu et al. 2014] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. \"Convolutional neural network architectures for matching natural language sentences.\" In Advances in neural information processing systems (NIPS), pp. 2042-2050. 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
